{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lab\n",
    "\n",
    "### This script demonstrates how to use a local LLM (in GGUF format) with llama-cpp-python and langchain to engineer better prompts for image generation. \n",
    "### The user provides a simple prompt, and the LLM rewrites it to be more detailed and suitable for text-to-image models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Required Packages\n",
    "import os\n",
    "import sys\n",
    "from llama_cpp import Llama\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: models/prompt_engineering/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf\n"
     ]
    }
   ],
   "source": [
    "# set root folder\n",
    "os.chdir('/Users/suvz47/Code/Cursor/NIAT Demo')  # Adjust this path if your project root is different\n",
    "\n",
    "# 2. Load the LLM Model (GGUF)\n",
    "# Set the directory where the GGUF model is stored\n",
    "model_dir = 'models/prompt_engineering'\n",
    "# Find the first split of the GGUF model\n",
    "model_files = [f for f in os.listdir(model_dir) if f.endswith('-00001-of-00002.gguf')]\n",
    "assert model_files, 'No first split GGUF model found in models/prompt_engineering/'\n",
    "model_path = os.path.join(model_dir, model_files[0])\n",
    "\n",
    "# Load the LLM using llama-cpp-python\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,      # Context window size\n",
    "    n_batch=128,     # Batch size for inference\n",
    "    verbose=False    # Suppress verbose output\n",
    ")\n",
    "print(f'Loaded model: {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define instructions for prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Prompt Engineering Chain with explicit tags, example, and clear instructions\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert prompt engineer for AI image generation.\n",
    "\n",
    "Your task is to rewrite the following user prompt to be more detailed, vivid, and creative, specifying style, lighting, composition, and any relevant details for a text-to-image model.\n",
    "\n",
    "Output the improved prompt inside <improved_prompt> and </improved_prompt> tags, and output only ONE improved prompt. Do not repeat or generate multiple improved prompts.\n",
    "\n",
    "Here is an example:\n",
    "User prompt: a dog in a park\n",
    "<improved_prompt>A photorealistic golden retriever joyfully running through a lush green park on a sunny afternoon, with soft sunlight filtering through tall trees, vibrant flowers in the background, \n",
    "and a blue sky overhead. The dog's fur glistens in the light, and its tongue is out in a playful expression.</improved_prompt>\n",
    "\n",
    "Now, here is the user prompt:\n",
    "User prompt: {user_prompt}\n",
    "<improved_prompt>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the Prompt Engineering Function (Stream, extract from tags)\n",
    "def engineer_prompt_stream(user_prompt, max_tokens=256, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Streams an improved image generation prompt using Qwen's chat format.\n",
    "    The model is instructed to rewrite the user's prompt to be more detailed and creative.\n",
    "    \"\"\"\n",
    "    # Qwen chat-style prompt with system and user roles\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant and an expert prompt engineer for AI image generation. \"\n",
    "        \"Your task is to rewrite the user's prompt to be more detailed, vivid, and creative, specifying style, lighting, composition, and any relevant details for a text-to-image model. \"\n",
    "        \"Output only one improved prompt.\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    # Call the model and stream the output as it is generated\n",
    "    stream = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        text = chunk['choices'][0]['text']\n",
    "        if text:\n",
    "            sys.stdout.write(text)\n",
    "            sys.stdout.flush()\n",
    "    print()  # Print a newline after streaming is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt Engineering for Image Generation (Terminal Mode)\n",
      "-----------------------------------------------------\n",
      "\n",
      "Engineered Prompt:\n",
      "------------------\n",
      "A majestic black cat wearing a tall, whimsical top hat with a colorful feather on the side, sitting confidently in a cozy armchair adorned with floral patterns. The lighting is soft and warm, casting gentle shadows and highlighting the cat's fluffy fur and expressive eyes. The background features a charming vintage wallpaper with intricate floral designs, adding to the enchanting atmosphere of a whimsical tale come to life.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nPrompt Engineering for Image Generation (Terminal Mode)\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    try:\n",
    "        while True:\n",
    "            # Prompt the user for an image description\n",
    "            user_prompt = input(\"\\nEnter a simple image prompt (Ctrl+C to exit): \")\n",
    "            print(\"\\nEngineered Prompt:\\n------------------\")\n",
    "            engineer_prompt_stream(user_prompt)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting. Thank you for using the prompt engineering demo!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
