{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lab\n",
    "\n",
    "### This script demonstrates how to use a local LLM (in GGUF format) with llama-cpp-python and langchain to engineer better prompts for image generation. \n",
    "### The user provides a simple prompt, and the LLM rewrites it to be more detailed and suitable for text-to-image models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Required Packages\n",
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "from llama_cpp import Llama\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define root path (you can set this to an absolute path or compute it)\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# 2. Load Configuration\n",
    "config = configparser.ConfigParser()\n",
    "config_path = os.path.join(root_path, 'config.ini')\n",
    "config.read(config_path)\n",
    "\n",
    "# Get LLM configuration\n",
    "relative_model_dir = config.get('LLM', 'model_dir_prompt')\n",
    "model_dir = os.path.join(root_path, relative_model_dir)\n",
    "n_ctx = config.getint('LLM', 'n_ctx')\n",
    "n_batch = config.getint('LLM', 'n_batch')\n",
    "max_tokens = config.getint('LLM', 'max_tokens')\n",
    "temperature = config.getfloat('LLM', 'temperature')\n",
    "\n",
    "# 3. Load the LLM Model (GGUF)\n",
    "# Find the first split of the GGUF model\n",
    "model_files = [f for f in os.listdir(model_dir) if f.endswith('-00001-of-00002.gguf')]\n",
    "assert model_files, f'No first split GGUF model found in {model_dir}/'\n",
    "model_path = os.path.join(model_dir, model_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from config.ini\n"
     ]
    }
   ],
   "source": [
    "def load_config():\n",
    "    \"\"\"\n",
    "    Load configuration from config.ini file.\n",
    "    \n",
    "    Returns:\n",
    "        configparser.ConfigParser: Configuration object\n",
    "    \"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config_path = '../config.ini'\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        config.read(config_path)\n",
    "        print(\"Configuration loaded from config.ini\")\n",
    "    else:\n",
    "        print(\"config.ini not found\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: /Users/suvz47/Code/Cursor/NIAT Demo/models/prompt_engineering/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf\n"
     ]
    }
   ],
   "source": [
    "# set root folder\n",
    "os.chdir('/Users/suvz47/Code/Cursor/NIAT Demo')  # Adjust this path if your project root is different\n",
    "\n",
    "# Load the LLM using llama-cpp-python\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=n_ctx,      # Context window size\n",
    "    n_batch=n_batch,  # Batch size for inference\n",
    "    verbose=False     # Suppress verbose output\n",
    ")\n",
    "print(f'Loaded model: {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define instructions for prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the Prompt Engineering Chain with explicit tags, example, and clear instructions\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert prompt engineer for AI image generation.\n",
    "\n",
    "Your task is to rewrite the following user prompt to be more detailed, vivid, and creative, specifying style, lighting, composition, and any relevant details for a text-to-image model.\n",
    "\n",
    "Output the improved prompt inside <improved_prompt> and </improved_prompt> tags, and output only ONE improved prompt. Do not repeat or generate multiple improved prompts.\n",
    "\n",
    "Here is an example:\n",
    "User prompt: a dog in a park\n",
    "<improved_prompt>A photorealistic golden retriever joyfully running through a lush green park on a sunny afternoon, with soft sunlight filtering through tall trees, vibrant flowers in the background, \n",
    "and a blue sky overhead. The dog's fur glistens in the light, and its tongue is out in a playful expression.</improved_prompt>\n",
    "\n",
    "Now, here is the user prompt:\n",
    "User prompt: {user_prompt}\n",
    "<improved_prompt>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create the Prompt Engineering Function (Stream, extract from tags)\n",
    "def engineer_prompt_stream(user_prompt, max_tokens=None, temperature=None):\n",
    "    # Use config values if not provided\n",
    "    if max_tokens is None:\n",
    "        max_tokens = config.getint('LLM', 'max_tokens')\n",
    "    if temperature is None:\n",
    "        temperature = config.getfloat('LLM', 'temperature')\n",
    "    \"\"\"\n",
    "    Streams an improved image generation prompt using Qwen's chat format.\n",
    "    The model is instructed to rewrite the user's prompt to be more detailed and creative.\n",
    "    \"\"\"\n",
    "    # Qwen chat-style prompt with system and user roles\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant and an expert prompt engineer for AI image generation. \"\n",
    "        \"Your task is to rewrite the user's prompt to be more detailed, vivid, and creative, specifying style, lighting, composition, and any relevant details for a text-to-image model. \"\n",
    "        \"Output only one improved prompt.\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    # Call the model and stream the output as it is generated\n",
    "    stream = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        text = chunk['choices'][0]['text']\n",
    "        if text:\n",
    "            sys.stdout.write(text)\n",
    "            sys.stdout.flush()\n",
    "    print()  # Print a newline after streaming is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt Engineering for Image Generation (Terminal Mode)\n",
      "-----------------------------------------------------\n",
      "\n",
      "Engineered Prompt:\n",
      "------------------\n",
      "A charming, well-groomed poodle wearing a vibrant, flowing red velvet dress adorned with tiny silver sequins, standing proudly in a sunlit garden during twilight, with soft golden hues casting gentle shadows. The composition highlights the dog's joyful expression and elegant pose, with lush greenery and blooming flowers in the background, creating a magical and whimsical scene.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nPrompt Engineering for Image Generation (Terminal Mode)\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    try:\n",
    "        while True:\n",
    "            # Prompt the user for an image description\n",
    "            user_prompt = input(\"\\nEnter a simple image prompt (Ctrl+C to exit): \")\n",
    "            print(\"\\nEngineered Prompt:\\n------------------\")\n",
    "            engineer_prompt_stream(user_prompt)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting. Thank you for using the prompt engineering demo!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
