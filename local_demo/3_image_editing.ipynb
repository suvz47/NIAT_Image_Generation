{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac8243b",
   "metadata": {},
   "source": [
    "# Image Editing with Prompt Engineering and Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821f193",
   "metadata": {},
   "source": [
    "## 1. Import Required Modules and Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd40e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('src/llm')  # Add LLM module to path\n",
    "from llm import generate_engineered_prompt\n",
    "import configparser\n",
    "from llama_cpp import Llama\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d5baa",
   "metadata": {},
   "source": [
    "## 2. Load LLM Configuration from config.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "model_dir = config.get('LLM', 'model_dir_prompt')\n",
    "n_ctx = config.getint('LLM', 'n_ctx')\n",
    "n_batch = config.getint('LLM', 'n_batch')\n",
    "max_tokens = config.getint('LLM', 'max_tokens')\n",
    "temperature = config.getfloat('LLM', 'temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a913cf3",
   "metadata": {},
   "source": [
    "## 3. Load GGUF Model with llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = [f for f in os.listdir(model_dir) if f.endswith('-00001-of-00002.gguf')]\n",
    "assert model_files, f'No first split GGUF model found in {model_dir}/'\n",
    "model_path = os.path.join(model_dir, model_files[0])\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=n_ctx,\n",
    "    n_batch=n_batch,\n",
    "    verbose=False\n",
    ")\n",
    "print(f'Loaded editing LLM model: {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad662ac3",
   "metadata": {},
   "source": [
    "## 4. Prompt Template for Image Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert prompt engineer for AI image editing.\n",
    "\n",
    "Your task is to rewrite the following user prompt to be more detailed, vivid, and creative, specifying style, lighting, composition, and any relevant details for an image-to-image editing model. The prompt should clearly describe how to transform the given image.\n",
    "\n",
    "Output the improved prompt inside <improved_prompt> and </improved_prompt> tags, and output only ONE improved prompt. Do not repeat or generate multiple improved prompts.\n",
    "\n",
    "Now, here is the user prompt:\n",
    "User prompt: {user_prompt}\n",
    "<improved_prompt>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0839a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_edit_prompt(user_prompt, max_tokens=None, temperature=None):\n",
    "    if max_tokens is None:\n",
    "        max_tokens = config.getint('LLM', 'max_tokens')\n",
    "    if temperature is None:\n",
    "        temperature = config.getfloat('LLM', 'temperature')\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant and an expert prompt engineer for AI image editing. \"\n",
    "        \"Your task is to rewrite the user's prompt to be more detailed, vivid, and creative, specifying style, lighting, composition, and any relevant details for an image-to-image editing model. \"\n",
    "        \"Output only one improved prompt.\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    response = llm(prompt, max_tokens=max_tokens, temperature=temperature)\n",
    "    text = response[\"choices\"][0][\"text\"].strip()\n",
    "    if \"<improved_prompt>\" in text and \"</improved_prompt>\" in text:\n",
    "        text = text.split(\"<improved_prompt>\",1)[1].split(\"</improved_prompt>\",1)[0].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c3ad8",
   "metadata": {},
   "source": [
    "## 5. Load Stable Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f42ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe = pipe.to(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec46ef",
   "metadata": {},
   "source": [
    "## 6. Select and Preprocess the Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaaf70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir('output') if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "if not image_files:\n",
    "    raise FileNotFoundError(\"No image files found in output directory\")\n",
    "\n",
    "for i, filename in enumerate(image_files, 1):\n",
    "    print(f\"{i}. {filename}\")\n",
    "\n",
    "choice = int(input(f\"\\nSelect image (1-{len(image_files)}): \"))\n",
    "image_path = os.path.join('output', image_files[choice - 1])\n",
    "init_image = Image.open(image_path).convert(\"RGB\").resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ba1c6",
   "metadata": {},
   "source": [
    "## 7. Apply Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = input(\"Enter your image editing prompt: \")\n",
    "try:\n",
    "    engineered_prompt = engineer_edit_prompt(user_prompt)\n",
    "    print(f\"\\nEngineered prompt for editing:\\n{engineered_prompt}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"[Warning] LLM prompt engineering failed: {e}\\nUsing original prompt.\")\n",
    "    engineered_prompt = user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c539b63",
   "metadata": {},
   "source": [
    "## 8. Perform Image Editing and Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipe(prompt=engineered_prompt, image=init_image, strength=0.7, guidance_scale=8).images\n",
    "output_path = f\"output/EDITED_{os.path.basename(image_path)}\"\n",
    "images[0].save(output_path)\n",
    "print(f\"\\nImage edited and saved as: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
